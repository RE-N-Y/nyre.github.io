---
layout: single
title: "End-to-End Machine Learning"
classes: wide
---

Ever since 2015, when Deepmind's AlphaGo triumped against Lee Sedol in Go -- after seeing what Machine Learing (ML) was capable of -- companies and governments started adopting ML into production. Many exisiting systems such as Google Translate, Netflix / YouTube's recommendation engine, and Alexa's speech recognition has seen significant improvements.

Over the years, ML has gotten accesible to the point where anyone with python installtion on their PC could run ML code on their computer. However, deploying a ML system from a local desktop to production is another story.

In this series of articles, I will go through how one deploy a local ML code into production as well as introducing additional challenge that ML pipelines introduce. Without further ado, let's get started!

## The ML Code

Let's say you have developed a simple python code **iris.py** which uses scikit-learn to classify Iris flowers based on
their sepal/petal dimenions.

For instance, if one would like to know flower with:

- Sepal Length: 3.0
- Sepal Width: 2.0
- Petal Length: 1.0
- Petal Width: 1.0

Running `python iris.py --sl 3.0 --sw 2.0 --pl 1.0 --pw 1.0` would print the classification.

```python
import argsparse
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

# Train model
X, y = load_iris(return_X_y=True)
clf = DecisionTreeClassifier()
clf = clf.fit(X, y)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_arugment("--sl", type=float)
    parser.add_arugment("--sw", type=float)
    parser.add_arugment("--pl", type=float)
    parser.add_arugment("--pw", type=float)
    args = parser.parse_args()

    data = [[args.sl, args.sw, args.pl, args.pw]]

    prediction = clf.predict(data)

    print(prediction)
```

You confirm that **iris.py** works on your machine and you're exicted to share this python program with everyone.

The question is **HOW**.

## Distributing ML Code

Now, there are a few options to share the ML code. The most common way to share code is through **GitHub**.
If anyone wants to use **iris.py**, they can download the code from Github and run the code on their own environment.

However, this approach has several limitations.

- **Environment inconsistancy**

Sure, the code works on **YOUR** machine, but how do you know if the code works on **THEIR** machine?
In case of **iris.py**, the user's computer may not even have python / scikit-learn installed. Even worse, if your code depends on external packages, custom binaries, or specific software versions, ensuring that your user has same runtime environment becomes increasingly difficult.

- **Sensitive Code / Data**

Although, open-sourcing code has become a common practice, it may be problematic if code contains service account credentials, and sensitive data in order for it to work. Moreover, open-sourcing may not be an ideal option if you wish to monetize your API.

- **Ease-of-use**

Even if the author includes installation instructions in README.md, users have to download the code and may have to install, compile, and build the program. These procedures are not only frustrating but can lead to possible failures during installation of necessary dependancies.

- **Integration**

Lastly, integrating the code with existing systems can prove difficult. Imagine if a owner of a flower shop wants to use **iris.py** on his website, but his backend is build with Node.js, which runs on javscript code. In this case, he would have to install python on his server, and figure out how he can run python code in javascript which can be a pain.

In most cases, **Github** might be a sufficient solution in most cases, but distributing **ML code's functionality to multiple runtime environments** require a different approach.

Or formally known as **IT WORKED ON MY MACHINE** problem.

![it worked on my machine...](/assets/images/worked_on_my_machine.jpg){: .align-center}

## REST API

REST API is a nice solution to escape limitations of manually distributing ML code. Since the code only runs on the server, you only need to make sure that the code is functional on one machine. Moreover, it lifts heavy-duty work of installing required depandancies for the users and allow developers to access the functionality via simple HTTP calls. In short, **REST API provides layer of abstraction** for ML code.

In fact, many of Google and Azure's ML solutions such as image classification, speech-to-text, and NLP APIs are accessible as REST API because it simplifies developers experience.

In this article, we will use [Flask](https://github.com/pallets/flask) and [gunicorn](https://gunicorn.org/) to transform our **iris.py** to a REST API.

**Flask** is a light-weight python library to build web APIs and **gunicorn** is a HTTP server runner.
Both can be easily installed via: `pip install flask gunicorn`

```python
from flask import Flask, jsonify
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

# Train model on initialization
X, y = load_iris(return_X_y=True)
clf = DecisionTreeClassifier()
clf = clf.fit(X, y)

app = Flask(__name__)

# Receive inputs via POST request
@app.route("/", methods=["POST"])
def predict():
    sl, sw, pl, pw = requests.json["sl"], requests.json["sw"], requests.json["pl"], requests.json["pw"]
    data = [[sl, sw, pl, pw]]
    prediction = clf.predict(data).tolist()

    # Return prediction as JSON
    return jsonify(prediction=prediction)
```

Simple right?

Now, we can use gunicorn to boot up the server locally!

```bash
gunicorn iris:app
[2020-12-03 19:49:24 +0900] [28547] [INFO] Starting gunicorn 20.0.4
[2020-12-03 19:49:24 +0900] [28547] [INFO] Listening at: http://127.0.0.1:8000 (28547)
[2020-12-03 19:49:25 +0900] [28547] [INFO] Using worker: sync
[2020-12-03 19:49:25 +0900] [28550] [INFO] Booting worker with pid: 28550
```

Now that our server is hosted on `http://127.0.0.1:8000`, let's give a little test drive and confirm that the API is invokable via HTTP request with `curl`

```bash
curl -H "Content-Type: application/json" \
    --request POST \
    --data '{ "sl": 2.0, "sw": 1.0, "pl": 2.0, "pw": 3.0 }' \
    127.0.0.1:8000
{"prediction":[1]}
```

Works like a charm!

## Considerations

However, there are drawbacks to this approach when it comes to sharing code's functionality.

**Firstly**, because source code and implementation details are hidden to the API users, it makes customization difficult for the end users.

**Secondly**, serving the code's functionality as web API introduces additional challenges specific to API management.

**Thirdly**, since users are not using their own computational resources to run your code, you need to ensure enough compute resources to meet with user's demand.

Hence, typical use cases for wrapping your code with REST API would be when:

- Your code requires heavy / complex required dependancies and build steps.
- Ease-of-use and consistent functionality.
- End users don't care about internal implementation and just want to use code's functions.
- No need for fine-grained customization.

On the other hand if your use case is:

- Your code requires light depandancies and only uses standard packages.
- End users are intersted in implementation details of your code.
- Code and its functionality should be customizable.

Then open-sourcing your code might be a good option.

## Takeaways

- Disparity between developer's run time and user's runtime acts as a barrier to sharing code's functionality.
- REST API can act as an layer of abstraction to simply both developer and API user's experience.
- Using Flask and Gunicorn to wrap existing python code to a web API service.

In our next article, we will look into how we can deploy our API from your local machine to Google Cloud Platform.
