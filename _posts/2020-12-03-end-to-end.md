---
layout: single
title: "Shipping ML"
classes: wide
---

Ever since 2015, when Deepmind's AlphaGo triumphed against Lee Sedol in Go -- after seeing what Machine Learning (ML) was capable of -- companies and governments started adopting ML into production. Many existing systems such as Google Translate, Netflix / YouTube's recommendation engine, and Alexa's speech recognition has seen significant improvements.

Over the years, ML has gotten accessible to the point where anyone with python installation on their PC could run ML code on their computer. However, deploying an ML system from a local desktop to production is another story.

In this series of articles, I will go through how one deploy a local ML code into production as well as introducing additional challenge that ML pipelines introduce. Without further ado, let's get started!

## The ML Code

Let's say you have developed a simple python code **iris.py** which uses scikit-learn to classify Iris flowers based on
their sepal/petal dimensions.

For instance, if one would like to know flower with:

- Sepal Length: 3.0
- Sepal Width: 2.0
- Petal Length: 1.0
- Petal Width: 1.0

Running `python iris.py --sl 3.0 --sw 2.0 --pl 1.0 --pw 1.0` would print the classification.

```python
import argsparse
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

# Train model
X, y = load_iris(return_X_y=True)
clf = DecisionTreeClassifier()
clf = clf.fit(X, y)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_arugment("--sl", type=float)
    parser.add_arugment("--sw", type=float)
    parser.add_arugment("--pl", type=float)
    parser.add_arugment("--pw", type=float)
    args = parser.parse_args()

    data = [[args.sl, args.sw, args.pl, args.pw]]

    prediction = clf.predict(data)

    print(prediction)
```

You confirm that **iris.py** works on your machine and you're excited to share this python program with everyone.

The question is **HOW**.

## Distributing ML Code

Now, there are a few options to share the ML code. The most common way to share code is through **GitHub**.
If anyone wants to use **iris.py**, they can download the code from Github and run the code on their own environment.

However, this approach has several limitations.

- **Environment inconsistency**

Sure, the code works on **YOUR** machine, but how do you know if the code works on **THEIR** machine?
In the case of **iris.py**, the user's computer may not even have python / scikit-learn installed. Even worse, if your code depends on external packages, custom binaries, or specific software versions, ensuring that your user has the same runtime environment becomes increasingly difficult.

- **Sensitive Code / Data**

Although open-sourcing code has become a common practice, it may be problematic if the code contains service account credentials and sensitive data for it to work. Moreover, open-sourcing may not be an ideal option if you wish to monetize your API.

- **Ease-of-use**

Even if the author includes installation instructions in README.md, users have to download the code and may have to install, compile, and build the program. These procedures are not only frustrating but can lead to possible failures during the installation of necessary dependencies.

- **Integration**

Lastly, integrating the code with existing systems can prove difficult. Imagine if an owner of a flower shop wants to use **iris.py** on his website, but his backend is built with Node.js, which runs on javascript code. In this case, he would have to install python on his server and figure out how he can run python code in javascript which can be a pain.

In most cases, **Github** might be a sufficient solution, but distributing **ML code's functionality to multiple runtime environments** require a different approach. Furthermore, it's important to note that problem of distributing code's functionality is not specific to ML code, but of software engineering in general.

Or formally known as **IT WORKED ON MY MACHINE** problem.

![it worked on my machine...]({{ site.url }}{{ site.baseurl }}/assets/images/worked_on_my_machine.jpg){: .align-center}

## REST API

REST API is a nice solution to escape the limitations of manually distributing ML code. Since the code only runs on the server, you only need to make sure that the code is functional on one machine. Moreover, it lifts heavy-duty work of installing required dependencies for the users and allows developers to access the functionality via simple HTTP calls. In short, **REST API provides a layer of abstraction** for ML code.

Many of Google and Azure's ML solutions such as image classification, speech-to-text, and NLP APIs are accessible as REST API because it simplifies developers' experience. Most recently, OpenAI decided to release [GPT-3](https://beta.openai.com/) as a web API powered by Microsoft Azure's infrastructure.

In this article, we will use [Flask](https://github.com/pallets/flask) and [gunicorn](https://gunicorn.org/) to transform our **iris.py** to a REST API.

**Flask** is a light-weight Python library to build web APIs and **gunicorn** is an HTTP server runner.
Both can be easily installed via: `pip install flask gunicorn`

```python
from flask import Flask, jsonify, request
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

# Train model on initialization
X, y = load_iris(return_X_y=True)
clf = DecisionTreeClassifier()
clf = clf.fit(X, y)

app = Flask(__name__)

# Receive inputs via POST request
@app.route("/", methods=["POST"])
def predict():
    sl, sw, pl, pw = request.json["sl"], request.json["sw"], request.json["pl"], request.json["pw"]
    data = [[sl, sw, pl, pw]]
    prediction = clf.predict(data).tolist()

    # Return prediction as JSON
    return jsonify(prediction=prediction)
```

Simple right?

Now, we can use gunicorn to boot up the server locally!

```bash
gunicorn iris:app
[2020-12-03 19:49:24 +0900] [28547] [INFO] Starting gunicorn 20.0.4
[2020-12-03 19:49:24 +0900] [28547] [INFO] Listening at: http://127.0.0.1:8000 (28547)
[2020-12-03 19:49:25 +0900] [28547] [INFO] Using worker: sync
[2020-12-03 19:49:25 +0900] [28550] [INFO] Booting worker with pid: 28550
```

Now that our server is hosted on `http://127.0.0.1:8000`, let's give a little test drive and confirm that the API is invokable via an HTTP request with `curl`

```bash
curl -H "Content-Type: application/json" \
    --request POST \
    --data '{ "sl": 2.0, "sw": 1.0, "pl": 2.0, "pw": 3.0 }' \
    127.0.0.1:8000
{"prediction":[1]}
```

Works like a charm!

## Considerations

However, there are drawbacks to this approach when it comes to sharing code's functionality.

**Firstly**, because source code and implementation details are hidden to the API users, it makes customization difficult for the end-users.

**Secondly**, serving the code's functionality as web API introduces additional challenges specific to API management.

**Thirdly**, since users are not using their computational resources to run your code, you need to ensure enough compute resources to meet with user's demand.

Hence, typical use cases for wrapping your code with REST API would be when:

- Your code requires heavy/complex required dependencies and build steps.
- Ease-of-use and consistent functionality.
- End users don't care about internal implementation and just want to use code's functions.
- No need for fine-grained customization.
- Private access to sensitive data/resources is needed.

On the other hand, if your use case is:

- Your code requires light dependencies and only uses standard packages.
- End users are interested in the implementation details of your code.
- Code and its functionality should be customizable.

Then open-sourcing your code might be a good option.

## Takeaways

- Disparity between developer's run time and user's runtime acts as a barrier to sharing code's functionality.
- REST API can act as a layer of abstraction to simply both developer and API user's experience.
- Using Flask and Gunicorn to wrap existing python code to a Web API service.

In our next article, we will look into how we can deploy our API from your local machine to Google Cloud Platform.
