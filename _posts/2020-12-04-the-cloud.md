---
layout: single
title: "The Cloud"
classes: wide
---

In the last article, we discussed different strategies for distributing ML code and created a minimal ML API for classifying flowers.
Now, let's dive into the deployment process for hosting ML API.

## Docker

![Docker]({{ site.url }}{{ site.baseurl }}/assets/images/docker.png){: .align-center .half-scale}

Before deploying our local API to a cloud computing service, we need to ensure that the server that will be running our API has a compatible runtime environment as our development environment. We will be using Docker to help us with this issue.

Since **iris.py** only relies on a handful of dependencies, we can write a minimal Dockerfile to replicate our development machine.

```bash
FROM python:3.7-slim
WORKDIR /app
ENV PORT 8080

# Install python dependencies
RUN pip install flask gunicorn
RUN pip install scikit-learn xgboost

# Copy code
COPY . .

CMD exec gunicorn --bind :$PORT iris:app
```

Now, let's bundle **iris.py** with Dockerfile to build out our container. First, let's wrap both files inside `/api` folder.

```bash
/api
├── iris.py
├── Dockerfile
```

Inside `/api`, we can build the docker container and let's spin up a container for our API.

```bash
# Build
docker build -t iris .

# Forward port 8080 to localhost:9090
docker run -p 9090:8080 -e PORT=8080 iris
[2020-12-04 01:54:47 +0000] [1] [INFO] Starting gunicorn 20.0.4
[2020-12-04 01:54:47 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)
[2020-12-04 01:54:47 +0000] [1] [INFO] Using worker: sync
[2020-12-04 01:54:47 +0000] [9] [INFO] Booting worker with pid: 9

# Test
curl -H "Content-Type: application/json" \
    --request POST \
    --data '{ "sl": 2.0, "sw": 1.0, "pl": 2.0, "pw": 3.0 }' \
    localhost:9090
{"prediction":[1]}
```

Great, now we are ready to deploy this container to a Cloud Provider.

## Cloud Run

[Cloud Run](https://cloud.google.com/run) is a computer service provided by GCP that we will use to deploy our container.
Note that AWS and Azure offer similar services like [AWS Fargate](https://aws.amazon.com/fargate/) and [Azure Container Instances](https://azure.microsoft.com/en-us/services/container-instances/) which helps you to easily deploy containers to their servers, but for this article, we shall use Clour Run.

To host a container to GCP, ensure that you have a [GCP account](https://cloud.google.com/) and [gcloud CLI](https://cloud.google.com/sdk) Additionally, ensure that you have **Cloud Build and Cloud Run API** enabled for your GCP project.

With these prerequisites, we can submit our build to Google Cloud Build and deploy our API to Cloud Run.

```bash
# Inside /api directory
gcloud builds submit --tag gcr.io/<PROJECT-ID>/api

# Deploy to us-east1 and allow unauthenticated calls
gcloud run deploy --image gcr.io/<PROJECT-ID>/api \
                  --platform managed \
                  --region us-east1 \
                  --allow-unauthenticated

# Test Deployed API endpoint with returned endpoint
curl -H "Content-Type: application/json" \
    --request POST \
    --data '{ "sl": 2.0, "sw": 1.0, "pl": 2.0, "pw": 3.0 }' \
    <ENDPOINT>
{"prediction":[1]}
```

## Looking Back

Taking a step back from our process, let's consider how all the components fit together.

1. Docker: Ensures runtime environment is consistent across different machines
2. REST API: Provide a Layer of abstraction to easily access code's functionality
3. Cloud Provider: Manages hardware and availability of REST API

Hence, at the end of the day, the developer only needs to manage:

1. Core python code
2. Flask wrapper code
3. DockerFile
4. Uploading/Submitting a Docker image to Cloud Provider

It's worthwhile to note that these solutions are not specific to ML/Python code, but of sharing software in general. Moreover, there are alternative solutions that AWS/Azure/GCP provides to easily share the code's functionality.

### Application Platforms

![app-engine]({{ site.url }}{{ site.baseurl }}/assets/images/app-engine.png){: .align-center .quarter-scale}

A solution to serve framework-specific applications (express.js, Django, Flask, Ruby on Rails, Laravel, etc) on a **standard runtime** provided by Cloud Provider.

A good use case would be:

1. Standard runtime environment
2. Ability to control API components

(e.g. e-commerce API, social media backend...)

Notable services include Heroku, GCP's App Engine, AWS's Elastic Beanstalk, and Azure's App Service.
The main advantage is that if you choose to use Cloud Provider's standard runtime, **it removes the necessity of Docker**, but you lose flexibility over the runtime environment.

### Function-as-a-Service (FaaS)

![cloud-functions]({{ site.url }}{{ site.baseurl }}/assets/images/cloud-functions.png){: .align-center .quarter-scale}

The solution to directly host "Functions" without writing API code. A developer simply submits functions written in their language to handle HTTP requests, and Cloud Providers automatically bundles it into an API endpoint.

A good use case would be:

1. Minimal required dependencies
2. Light-weight computation
3. Only performs 1~2 straight-forward functionalities

(e.g. File conversion API, Online Image Resizer...)

Notable services include GCP's Cloud Functions, AWS's Lambda Functions, and Azure's Azure Functions.
FaaS allows developers to **sorely** focus on core code's functionality because it also manages API-level components for you. Also, FaaS removes the need for API-layer like Flask and runtime management via Docker.
However, the limitation of FaaS is that you're going to have fairly limited flexibility on your runtime.
Many Cloud Providers will enforce certain versions of the programming language's runtime and installed packages to ensure availability and compatibility.

"There ain't no such thing as a free lunch". Convenience comes at a cost of flexibility.

## Next Steps

Now that we've learned different strategies and methods to opening up ML code's functionality, in our next article we will go over challenges that **ML** specifically introduces to the software development process.
